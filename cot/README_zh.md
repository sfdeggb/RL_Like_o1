### 📚 使用 gemma-2-2b-it 在 Groq 上创建 o1 推理链

这是一个使用提示策略提高 LLM 推理能力的早期原型，以创建类似于 o1 的推理链。该应用程序使用开源模型来创建“思考”并解决通常其他方法难以解决的逻辑问题。与 o1 不同，所有推理标记都显示，并且该应用程序使用开源模型。

g1 是实验性的，开源的，以帮助激发开源社区开发新的策略来产生类似于 o1 的推理链。这个实验有助于展示提示策略的潜力，使现有的开源模型能够受益于动态推理链和改进的界面，以探索它们。

### 🚀 它是如何工作的(WORK)

g1 使用gemma-2-2b-it 创建推理链，本质上是一个动态的 Chain of Thought，允许 LLM “思考”并解决一些通常难以解决的逻辑问题。

在每一步，LLM 可以选择继续到另一个推理步骤，或提供最终答案。每一步都显示给用户，标题为“思考”。系统提示还包括对 LLM 的提示，例如“包括探索替代答案的探索”和“使用至少 3 种方法推导答案”。

因此，LLM 的推理能力通过结合 Chain-of-Thought 与要求尝试多种方法、探索替代答案、质疑先前的草稿解决方案以及考虑 LLM 的局限性来提高。仅通过提示，而不进行任何训练，就足以在草莓问题（n=10，“草莓中有多少个 R？”）上达到 ~70% 的准确率。没有提示，Llama-3.1-70b 的准确率为 0%，ChatGPT-4o 的准确率为 30%。

### 📑 例子

> [!IMPORTANT]
> g1 不是完美的，但它可以比现有的 LLM 做得更好。从初步测试来看，g1 在简单逻辑问题上准确率约为 60-80%，通常使 LLM 难以解决。然而，准确率尚未正式评估。请参阅下面的示例。

##### 草莓中有多少个 R？

提示：草莓中有多少个 R？

结果：

![草莓示例](examples/strawberry.png)

---

提示：哪个更大，0.9 还是 0.11？

结果：

![0.9 或 0.11 示例](examples/math.png)

### 快速开始

要使用 Streamlit UI，请按照以下说明操作：

~~~
python3 -m venv venv
~~~

~~~
source venv/bin/activate
~~~

~~~
pip3 install -r requirements.txt
~~~

~~~
export GROQ_API_KEY=gsk...
~~~

~~~
streamlit run app.py
~~~

---

或者，按照以下额外说明使用 Gradio UI：

~~~
cd gradio
~~~

~~~
pip3 install -r requirements.txt
~~~

~~~
python3 app.py
~~~

### 🛡️提示策略

```
你是一个专业的人工智能助手，一步一步地解释你的推理。对于每个步骤，提供一个标题，描述您在该步骤中所做的工作以及内容。决定你是否需要下一步，或者你是否准备好给出最终答案。以JSON格式响应'title'， 'content'和'next_action' ('continue'或'final_answer')键。使用尽可能多的推理步骤。至少3个。要意识到你作为LLM的局限性，以及你能做什么和不能做什么。在你的推理中，包括对其他答案的探索。考虑你可能是错的，如果你的推理是错的，它会在哪里。充分测试所有其他可能性。你可能是错的。当你说你在重新检查时，实际上是重新检查，并且使用另一种方法。不要只是说你在重新审视。用至少3种方法推导出答案。使用最佳实践。
合乎规则的JSON回答例子：
json
{
    "title": "Identifying Key Information",
    "content": "To begin solving this problem, we need to carefully examine the given information and identify the crucial elements that will guide our solution process. This involves...",
    "next_action": "continue"
}
```

# 🌍分解
#### 分解

首先，添加一个角色设定：

> 你是一个专家级人工智能助手，能够逐步解释你的推理过程。

然后，给出描述预期的逐步推理过程的指示，同时为每个推理步骤添加标题。这包括让LLM决定是否需要另一个推理步骤，或者是否可以提供最终答案。

> 对于每个步骤，提供一个描述你在该步骤中所做工作的标题，以及内容。决定你是否需要另一个步骤或者是否准备好给出最终答案。

引入JSON格式，并在后面提供一个示例。

> 以JSON格式响应，包含'title'、'content'和'next_action'（'continue'或'final_answer'）键。

为了强调指令的重要性并提高提示符的遵从度，以全大写字母列出了一系列提示和最佳实践。

1. 尽可能使用多个推理步骤。至少3个。-> 这确保LLM真正花时间先思考，通常会产生约5-10个步骤。
2. 要意识到你作为LLM的局限性，以及你能做什么和不能做什么。-> 这有助于LLM记住使用能产生更好结果的技术，比如在计数之前将"strawberry"分解成单个字母。
3. 在你的推理中，包括对其他答案的探索。考虑你可能是错的，如果你的推理是错的，它会在哪里。-> 收益的很大一部分似乎来自LLM重新评估其初始响应，以确保它在逻辑上与问题一致。
4. 当你说你在重新检查时，实际上要重新检查，并使用另一种方法来做。不要只是说你在重新检查。-> 这鼓励防止LLM只是说它重新检查了问题而没有实际尝试新方法。
5. 使用至少3种方法来得出答案。-> 这通过尝试多种方法来帮助LLM得出正确答案。
6. 使用最佳实践。-> 这就像改善LLM代码输出的"做得更好"提示一样简单。通过告诉LLM使用最佳实践或做得更好，它通常会表现得更好！

> 使用尽可能多的推理步骤。至少3个。要意识到你作为LLM的局限性，以及你能做什么和不能做什么。在你的推理中，包括对其他答案的探索。考虑你可能是错的，如果你的推理是错的，它会在哪里。充分测试所有其他可能性。你可能是错的。当你说你在重新检查时，实际上要重新检查，并使用另一种方法来做。不要只是说你在重新检查。使用至少3种方法来得出答案。使用最佳实践。

最后，在将问题添加为用户消息后，加载一个助手消息，为LLM的生成提供一个标准化的起点。

> 助手：谢谢！我现在将按照我的指示一步一步地思考，在分解问题后从头开始。
